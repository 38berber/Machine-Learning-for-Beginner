# -*- coding: utf-8 -*-
"""ML_sunum.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18wQSl9qQLLzpYKhfhK2yyAXLjdVG6fEb

Wisconsin üniversitesinin radyolojik görüntüler üzerinden hazırladığı bir veri seti
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.metrics import r2_score, f1_score, recall_score, roc_auc_score, roc_curve, auc, accuracy_score, mean_squared_error, precision_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import matthews_corrcoef

dataset = pd.read_csv("breastCancer.csv")
data = dataset.drop(columns=['id'],inplace=True)
display(dataset)
display(data)
print("\n\n İyi Huylu - 2, Kötü Huylu - 4\n", dataset["class"].value_counts());
dataset["class"].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%', shadow=True)
plt.title("number of patients");

dataset.dtypes

"""Burada veri setindeki kolonların ne tür veri içerdiğini kontrol ediyoruz ve bare_nucleoli'de bir sıkıntı var kayıp veriyi kontrol etmeliyiz"""

dataset['bare_nucleoli'] = dataset['bare_nucleoli'].replace('?', np.nan)
dataset.isna().sum()

"""Kayıp veri için '?' yerine 'NaN' değerini atarak isna ile toplam kaçtane kayıp veri olduğunu buluyoruz"""

dataset = dataset.dropna(subset=['bare_nucleoli'])
dataset['bare_nucleoli'] = pd.to_numeric(dataset['bare_nucleoli'])

"""# NaN değerine sahip sütunları tamamen siliyoruz. Böylece verisetindeki farklılık ortadan kalkmış oluyor

"""

dataset.dtypes

data = dataset.to_numpy()
X = data[:,:-1];
Y = data[:,-1];
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.15, random_state=0)
estimator_BC = BaggingClassifier()
parameters_BC = {
    'n_estimators':(10,30,50),
    'max_samples': (1,2,3),
    'max_features':(1,2,3),
              }
grid_search_BC = GridSearchCV(
    estimator = estimator_BC,
    param_grid = parameters_BC,
    scoring = 'accuracy',
    n_jobs = -1,
    cv = 5
)
grid_search_BC.fit(X_train, Y_train)
best_params_BC = grid_search_BC.best_params_
best_score_BC = grid_search_BC.best_score_
print("En BC:", best_params_BC)
print("En iyi BC doğruluk skoru:", best_score_BC)

"""Bu kısımda gridsearch ile Baggingclassifier parametrelerinden doğruluğu en yüksek veren  değerler belirlenir ve ekrana yazdırılır. En iyi doğruluk değerleri ile diğer değerleri kontrol etmek içinse"""

best_estimators_BC = grid_search_BC.best_params_['n_estimators']
best_max_samples_BC = grid_search_BC.best_params_['max_samples']
best_max_features_BC = grid_search_BC.best_params_['max_features']
print('Best n_estimators: ', best_estimators_BC)
print('Best max_samples: ', best_max_samples_BC)
print('Best max features: ', best_max_features_BC)

"""Bu değerleri kullanarak tekrar eğitim işlemini gerçekleştirirsek doğruluk nasıl bir durum izleyecek"""

clf_BC = BaggingClassifier(n_estimators  = best_estimators_BC,
                             max_samples = best_max_samples_BC,
                             max_features = best_max_features_BC,
                             n_jobs = -1,  ).fit(X_train, Y_train)
Y_pred_BC = clf_BC.predict(X_test)
print("Bagging Classifier \n",confusion_matrix(Y_test, Y_pred_BC))
Y_pred_BC = clf_BC.predict(X_test)
Y_pred_BC = np.round(Y_pred_BC).astype(int)
print("MSE BC:", mean_squared_error(Y_test, Y_pred_BC))
accuracy_BC = accuracy_score(Y_test, Y_pred_BC)
print("Accuracy BC: %f" % accuracy_BC)
precision_BC = precision_score(Y_test, Y_pred_BC, pos_label=4)
print("Precision BC: %f" % precision_BC)
recall_BC = recall_score(Y_test, Y_pred_BC, pos_label=4)
print("Recall BC: %f" % recall_BC)
f1_BC = f1_score(Y_test, Y_pred_BC, pos_label=4)
print("F1 score BC: %f" % f1_BC)
mcc_BC = matthews_corrcoef(Y_test, Y_pred_BC)
print("MCC score BC: %f" % mcc_BC)
r2_BC= r2_score(Y_test, Y_pred_BC)
print("r2 score BC: %f" % r2_BC)
auc_BC = roc_auc_score(Y_test, Y_pred_BC)
print("ROC AUC BC: %f" % auc_BC)
ConfusionMatrixDisplay.from_predictions(Y_test, Y_pred_BC)
plt.show()

"""Burda kodu her yeniden çalıştırdığımızda veriler tamamen değişiyor"""

fpr_BC, tpr_BC, thresholds_BC = roc_curve(Y_test, Y_pred_BC, pos_label=4)
roc_auc_BC = auc(fpr_BC, tpr_BC)

# Plot ROC curve
plt.figure(figsize=(5, 3))
plt.plot(fpr_BC, tpr_BC, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_BC)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Bagging Classifier')
plt.legend(loc="lower right")
plt.show()

"""Farklı bir yöntem ile devam edelim"""

estimator_DT = DecisionTreeClassifier()
parameters_DT = {
    'criterion':('gini','entropy','log_loss'),
    'splitter': ('best', 'random'),
    'max_depth':(1,5,15),
    'min_samples_split': (0.000001, 0.00001, 0.0001),
              }
grid_search_DT = GridSearchCV(
    estimator = estimator_DT,
    param_grid = parameters_DT,
    scoring = 'accuracy',
    n_jobs = -1,
    cv = 5
)
grid_search_DT.fit(X_train, Y_train)
best_params_DT = grid_search_DT.best_params_
best_score_DT = grid_search_DT.best_score_
print("En DT:", best_params_DT)
print("En iyi DT doğruluk skoru:", best_score_DT)

best_criterion_DT = grid_search_DT.best_params_['criterion']
best_splitter_DT = grid_search_DT.best_params_['splitter']
best_max_depth_DT = grid_search_DT.best_params_['max_depth']
best_min_samples_split_DT = grid_search_DT.best_params_['min_samples_split']
print('Best criterion: ', best_criterion_DT)
print('Best splitter: ', best_splitter_DT)
print('Best max depth: ', best_max_depth_DT)
print('Best min samples: ', best_min_samples_split_DT)

clf_DT = DecisionTreeClassifier(criterion  = best_criterion_DT,
                             splitter = best_splitter_DT,
                             max_depth = best_max_depth_DT,
                             min_samples_split = best_min_samples_split_DT).fit(X_train, Y_train)
Y_pred_DT = clf_DT.predict(X_test)
print("DECISION TREE CLASSIFIER \n",confusion_matrix(Y_test, Y_pred_DT))
Y_pred_DT = clf_DT.predict(X_test)
Y_pred_DT = np.round(Y_pred_DT).astype(int)
print("MSE DT:", mean_squared_error(Y_test, Y_pred_DT))
accuracy_DT = accuracy_score(Y_test, Y_pred_DT)
print("Accuracy DT: %f" % accuracy_DT)
precision_DT = precision_score(Y_test, Y_pred_DT, pos_label=4)
print("Precision DT: %f" % precision_DT)
recall_DT = recall_score(Y_test, Y_pred_DT, pos_label=4)
print("Recall DT: %f" % recall_DT)
f1_DT = f1_score(Y_test, Y_pred_DT, pos_label=4)
print("F1 score DT: %f" % f1_DT)
mcc_DT = matthews_corrcoef(Y_test, Y_pred_DT)
print("MCC score DT: %f" % mcc_DT)
r2_DT= r2_score(Y_test, Y_pred_DT)
print("r2 score DT: %f" % r2_DT)
auc_DT = roc_auc_score(Y_test, Y_pred_DT)
print("ROC AUC DT: %f" % auc_DT)
ConfusionMatrixDisplay.from_predictions(Y_test, Y_pred_DT)
plt.show()

fpr_DT, tpr_DT, thresholds_DT = roc_curve(Y_test, Y_pred_DT, pos_label=4)
roc_auc_DT = auc(fpr_DT, tpr_DT)

# Plot ROC curve
plt.figure(figsize=(5, 3))
plt.plot(fpr_DT, tpr_DT, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_DT)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Decision Tree Classifier')
plt.legend(loc="lower right")
plt.show()

estimator_RF = RandomForestClassifier()
parameters_RF = {
    'n_estimators':(50,150,1),
    'criterion':('gini','entropy'),
    'max_depth':(10,160,1),
    'min_samples_split':(0.000001, 0.00001, 0.0001),
              }
grid_search_RF = GridSearchCV(
    estimator = estimator_RF,
    param_grid = parameters_RF,
    scoring = 'accuracy',
    n_jobs = -1,
    cv = 5
)

grid_search_RF.fit(X_train, Y_train)
best_params_RF = grid_search_RF.best_params_
best_score_RF = grid_search_RF.best_score_
print("En RF:", best_params_RF)
print("En iyi RF doğruluk skoru:", best_score_RF)

best_estimators_RF = grid_search_RF.best_params_['n_estimators']
best_criterion_RF = grid_search_RF.best_params_['criterion']
best_max_depth_RF = grid_search_RF.best_params_['max_depth']
best_sample_split_RF = grid_search_RF.best_params_['min_samples_split']
print(best_estimators_RF)
print(best_criterion_RF)
print(best_max_depth_RF)
print(best_sample_split_RF)

clf_RF = RandomForestClassifier(n_estimators  =best_estimators_RF,
                             criterion = best_criterion_RF,
                             max_depth = best_max_depth_RF,
                             min_samples_split = best_sample_split_RF,
                             n_jobs = -1,  ).fit(X_train, Y_train)
Y_pred_RF = clf_RF.predict(X_test)
print("Random Forest \n",confusion_matrix(Y_test, Y_pred_RF))
Y_pred_RF = clf_RF.predict(X_test)
Y_pred_RF = np.round(Y_pred_RF).astype(int)
print("MSE RF:", mean_squared_error(Y_test, Y_pred_RF))
accuracy_RF = accuracy_score(Y_test, Y_pred_RF)
print("Accuracy RF: %f" % accuracy_RF)
precision_RF = precision_score(Y_test, Y_pred_RF, pos_label=4)
print("Precision RF: %f" % precision_RF)
recall_RF = recall_score(Y_test, Y_pred_RF, pos_label=4)
print("Recall RF: %f" % recall_RF)
f1_RF = f1_score(Y_test, Y_pred_RF, pos_label=4)
print("F1 score RF: %f" % f1_RF)
mcc_RF = matthews_corrcoef(Y_test, Y_pred_RF, )
print("MCC score RF: %f" % mcc_RF)
r2_RF= r2_score(Y_test, Y_pred_RF)
print("r2 score RF: %f" % r2_RF)
auc_RF = roc_auc_score(Y_test, Y_pred_RF)
print("ROC AUC RF: %f" % auc_RF)
ConfusionMatrixDisplay.from_predictions(Y_test, Y_pred_RF)
plt.show()

fpr_RF, tpr_RF, thresholds_RF = roc_curve(Y_test, Y_pred_RF, pos_label=4)
roc_auc_RF = auc(fpr_RF, tpr_RF)

# Plot ROC curve
plt.figure(figsize=(5, 3))
plt.plot(fpr_RF, tpr_RF, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_RF)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Random Forest Classifier')
plt.legend(loc="lower right")
plt.show()

models = ['Bagging Classifier', 'Decision Tree', 'Random Forest']
accuracy = [accuracy_BC, accuracy_DT, accuracy_RF]
precision = [precision_BC, precision_DT, precision_RF]
recall = [recall_BC, recall_DT, recall_RF]
f1_score = [f1_BC, f1_DT, f1_RF]
roc_auc = [roc_auc_BC, roc_auc_DT, roc_auc_RF]

bar_width = 0.2

r1 = np.arange(len(models))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]
r4 = [x + bar_width for x in r3]
r5 = [x + bar_width for x in r4]

plt.bar(r1, accuracy, width=bar_width, label='Accuracy')
plt.bar(r2, precision, width=bar_width, label='Precision')
plt.bar(r3, recall, width=bar_width, label='Recall')
plt.bar(r4, f1_score, width=bar_width, label='F1 Score')
plt.bar(r5, roc_auc, width=bar_width, label='ROC AUC')

plt.xlabel('Models', fontweight='bold')
plt.xticks([r + bar_width * 2 for r in range(len(models))], models)

plt.title('Model Performans Karşılaştırması')
plt.ylabel('Score')

plt.legend()

plt.show()